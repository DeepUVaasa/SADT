{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 13:51:37.172452: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-21 13:51:37.703636: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64\n",
      "2022-09-21 13:51:37.703678: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.6/lib64\n",
      "2022-09-21 13:51:37.703684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/reza/.local/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "plt.rcParams['figure.figsize'] = 8,8\n",
    "\n",
    "import time\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "from tensorflow.keras.utils import *\n",
    "\n",
    "from tensorflow.keras.regularizers import *\n",
    "#import keras\n",
    "import numpy as np\n",
    "import os, math\n",
    "from matplotlib.pyplot import figure\n",
    "#from keras.backend import tensorflow_backend\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import math\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "#from tensorflow.python import debug as tf_debug\n",
    "import imageio\n",
    "import glob\n",
    "from skimage import transform as tf\n",
    " \n",
    "\n",
    "\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as plt_img\n",
    "import scipy\n",
    "import scipy\n",
    "import skimage\n",
    "import re\n",
    "#import LRFinder\n",
    "import math as m\n",
    "import tensorflow as tf  \n",
    "from pathlib import Path\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "import numpy as np\n",
    " \n",
    " \n",
    " \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "tf.random.Generator = None \n",
    " \n",
    " \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.fftpack import dct, idct\n",
    "import copy\n",
    "\n",
    " \n",
    "\n",
    "from numpy import linalg as la\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change here and select your preferred Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "#(x_train, y_train), (X_test, Y_test) = tf.keras.datasets.cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train),  len(X_test), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "y_train = y_train[0:len(x_train)] \n",
    "Y_test=  Y_test [0:len(X_test)] \n",
    "\n",
    "\n",
    "print(y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image_size = x_train.shape[1]\n",
    "\n",
    "x_train = np.reshape(x_train,[-1, image_size, image_size, 3])\n",
    "X_test = np.reshape(X_test,[-1, image_size, image_size, 3])\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(x_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "x_val=X_test[:3000]\n",
    "y_val=Y_test[:3000]\n",
    "x_test=X_test[3001:]\n",
    "y_test=Y_test[3001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_shape = (image_size, image_size, 3)\n",
    " \n",
    "kernel_size = 3\n",
    "filters = 64\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(units, dropout=0.2, activation='relu', block=1, layer=1):\n",
    "\n",
    "    def layer_wrapper(inp):\n",
    "        x = Conv2D(units, (3, 3), padding='same' ,kernel_initializer='he_normal', name='block{}_conv{}'.format(block, layer))(inp)\n",
    "        x = BatchNormalization(name='block{}_bn{}'.format(block, layer))(x)\n",
    "        x = Activation(activation, name='block{}_act{}'.format(block, layer))(x)\n",
    "        x = Dropout(dropout, name='block{}_dropout{}'.format(block, layer))(x)\n",
    "        return x\n",
    "\n",
    "    return layer_wrapper\n",
    "\n",
    "def dense_block(units, dropout=0.2, activation='relu', name='fc1'):\n",
    "\n",
    "    def layer_wrapper(inp):\n",
    "        x = Dense(units,kernel_initializer='he_normal', name=name)(inp)\n",
    "        x = BatchNormalization(name='{}_bn'.format(name))(x)\n",
    "        x = Activation(activation, name='{}_act'.format(name))(x)\n",
    "        x = Dropout(dropout, name='{}_dropout'.format(name))(x)\n",
    "        return x\n",
    "\n",
    "    return layer_wrapper\n",
    "        \n",
    "\n",
    "def VGG16_BN(input_tensor=None, input_shape=None, classes=1000, conv_dropout=0.1, dropout=0.3, activation='relu'):\n",
    "    \"\"\"Instantiates the VGG16 architecture with Batch Normalization\n",
    "    # Arguments\n",
    "        input_tensor: Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n",
    "        input_shape: shape tuple\n",
    "        classes: optional number of classes to classify images\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    \"\"\"\n",
    "    img_input = Input(shape=input_shape) if input_tensor is None else (\n",
    "        Input(tensor=input_tensor, shape=input_shape) if not K.is_keras_tensor(input_tensor) else input_tensor\n",
    "    )\n",
    "\n",
    "    # Block 1\n",
    "    x = conv_block(32, dropout=conv_dropout, activation=activation  , block=1, layer=1)(img_input)\n",
    "    x = conv_block(32, dropout=conv_dropout, activation=activation  , block=1, layer=2)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = conv_block(64, dropout=conv_dropout, activation=activation  , block=2, layer=1)(x)\n",
    "    x = conv_block(64, dropout=conv_dropout, activation=activation  , block=2, layer=2)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = conv_block(128, dropout=conv_dropout, activation=activation , block=3, layer=1)(x)\n",
    "    x = conv_block(128, dropout=conv_dropout, activation=activation , block=3, layer=2)(x)\n",
    "    x = conv_block(128, dropout=conv_dropout, activation=activation , block=3, layer=3)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = conv_block(256, dropout=conv_dropout, activation=activation,  block=4, layer=1)(x)\n",
    "    x = conv_block(256, dropout=conv_dropout, activation=activation,  block=4, layer=2)(x)\n",
    "    x = conv_block(256, dropout=conv_dropout, activation=activation,  block=4, layer=3)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = conv_block(256, dropout=conv_dropout, activation=activation,  block=5, layer=1)(x)\n",
    "    x = conv_block(256, dropout=conv_dropout, activation=activation,  block=5, layer=2)(x)\n",
    "    x = conv_block(256, dropout=conv_dropout, activation=activation,  block=5, layer=3)(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    # Flatten\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # FC Layers\n",
    "    x = dense_block(512, dropout=dropout, activation=activation ,  name='fc1')(x)\n",
    "    x = dense_block(512, dropout=dropout, activation=activation , name='fc2')(x)\n",
    "    \n",
    "    # Classification block    \n",
    "    x = Dense(classes, activation='relu' )(x)\n",
    "    x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
    "    inputs = get_source_inputs(input_tensor) if input_tensor is not None else img_input\n",
    "\n",
    "    # Create model.\n",
    "    return Model(inputs, x, name='vgg16_bn')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def InceptionResNetModule(input_shapee, n_filters, kernel_size):\n",
    "    seed=32\n",
    "    path_a = Conv2D(filters=n_filters/32, kernel_size=(3,3), padding='same',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=seed))(input_shapee)\n",
    "    path_a = BatchNormalization(axis=-1)(path_a)\n",
    "    path_a = Activation('elu')(path_a)\n",
    "\n",
    "    path_b = Conv2D(filters=n_filters, kernel_size=kernel_size, padding='same',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=seed))(input_shapee)\n",
    "    path_b = BatchNormalization(axis=-1)(path_b)\n",
    "    path_b = Activation('elu')(path_b)\n",
    "\n",
    "    path_b = Conv2D(filters=n_filters, kernel_size=kernel_size, padding='same',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=seed))(path_b)\n",
    "    path_b = BatchNormalization(axis=-1)(path_b)\n",
    "\n",
    "    shortcut = Conv2D(filters=n_filters, kernel_size=(3,3),  activation='elu', padding='same',\n",
    "               kernel_initializer=tf.keras.initializers.he_normal(seed=seed))(input_shapee)\n",
    "    shortcut = BatchNormalization(axis=-1)(shortcut)\n",
    "    shortcut = Activation('elu')(shortcut)\n",
    "\n",
    "    path_b = Activation('elu')(add([shortcut, path_b]))\n",
    "\n",
    "    path_c = Conv2D(filters=n_filters/32, kernel_size=(3, 3), padding='same',\n",
    "                    kernel_initializer=tf.keras.initializers.he_normal(seed=seed))(input_shapee)\n",
    "    path_c = BatchNormalization(axis=-1)(path_c)\n",
    "    path_c = Activation('elu')(path_c)\n",
    "\n",
    "    output = concatenate([path_a, path_b, path_c])\n",
    "\n",
    "    output = MaxPooling2D(pool_size=(2, 2))(output)\n",
    "    y = Dropout(0.2)(output)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def cir(input_shape,  classes=Y_test.shape[1]):\n",
    "    inputs = Input(shape=(input_shape))\n",
    "    kernel_size = (3,3)\n",
    "    y = Conv2D(filters=32, kernel_size=(3,3), kernel_initializer=tf.keras.initializers.he_normal(seed=32))(inputs)\n",
    "    y = InceptionResNetModule(y, 32, kernel_size)\n",
    "    y = InceptionResNetModule(y, 64, kernel_size)\n",
    "    y = InceptionResNetModule(y, 64, kernel_size)\n",
    "    y = Flatten()(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    y = Dense(128, activation='elu')(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    y = Dense(classes, activation='relu' )(y)\n",
    "    outputs = Dense(classes, activation='softmax')(y)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def noz(x):\n",
    "    x=np.asarray(x)\n",
    "    return (x-x.min())/(x.max()-x.min())\n",
    "\n",
    "\n",
    "def noma(x):\n",
    "    return (x-K.min(x))/(K.max(x)-K.min(x))\n",
    "\n",
    "\n",
    "\n",
    "def ResidualBlock(x, filters, kernel_size, weight_decay, downsample=True):\n",
    "    if downsample:\n",
    "        # residual_x = conv2d_bn_relu(x, filters, kernel_size=1, strides=2)\n",
    "        residual_x = conv2d_bn(x, filters, kernel_size=1, strides=2)\n",
    "        stride = 2\n",
    "    else:\n",
    "        residual_x = x\n",
    "        stride = 1\n",
    "    residual = conv2d_bn_relu(x,\n",
    "                              filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              weight_decay=weight_decay,\n",
    "                              strides=stride,\n",
    "                              )\n",
    "    residual = conv2d_bn(residual,\n",
    "                         filters=filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         weight_decay=weight_decay,\n",
    "                         strides=1,\n",
    "                         )\n",
    "    out =  add([residual_x, residual])\n",
    "    out = Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def conv2d_bn(x, filters, kernel_size, weight_decay=.0, strides=(1, 1)):\n",
    "    layer = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same',\n",
    "                   use_bias=False,\n",
    "                   kernel_regularizer=l2(weight_decay)\n",
    "                   )(x)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv2d_bn_relu(x, filters, kernel_size, weight_decay=.0, strides=(1, 1)):\n",
    "    layer = conv2d_bn(x, filters, kernel_size, weight_decay, strides)\n",
    "    layer = Activation('relu')(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "\n",
    "def ResNet18(classes, input_shape, weight_decay=1e-4):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = input\n",
    "    # x = conv2d_bn_relu(x, filters=64, kernel_size=(7, 7), weight_decay=weight_decay, strides=(2, 2))\n",
    "    # x = MaxPool2D(pool_size=(3, 3), strides=(2, 2),  padding='same')(x)\n",
    "    x = conv2d_bn_relu(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, strides=(1, 1))\n",
    "\n",
    "    # # conv 2\n",
    "    x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    # # conv 3\n",
    "    x = ResidualBlock(x, filters=128, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "    x = ResidualBlock(x, filters=128, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    # # conv 4\n",
    "    x = ResidualBlock(x, filters=256, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "    x = ResidualBlock(x, filters=256, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    # # conv 5\n",
    "    x = ResidualBlock(x, filters=512, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "    x = ResidualBlock(x, filters=512, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    x = AveragePooling2D(pool_size=(4, 4), padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='relu')(x)\n",
    "    x = Dense(classes, activation='softmax')(x)\n",
    "    model = Model(input, x, name='ResNet18')\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNetForCIFAR10(classes, name, input_shape, block_layers_num=5, weight_decay=1e-5):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = input\n",
    "    x = conv2d_bn_relu(x, filters=8, kernel_size=(3, 3), weight_decay=weight_decay, strides=(1, 1))\n",
    "\n",
    "    # # conv 2\n",
    "    for i in range(block_layers_num):\n",
    "        x = ResidualBlock(x, filters=8, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    # # conv 3\n",
    "    x = ResidualBlock(x, filters=32, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "    for i in range(block_layers_num - 1):\n",
    "        x = ResidualBlock(x, filters=32, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    # # conv 4\n",
    "    x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "    for i in range(block_layers_num - 1):\n",
    "        x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "    x = AveragePooling2D(pool_size=(8, 8), padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes, activation='relu')(x)\n",
    "    x = Dense(classes, activation='softmax')(x)\n",
    "    model = Model(input, x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def densenet(input_shape,classes_num):\n",
    "    growth_rate        = 3\n",
    "    depth              = 100\n",
    "    compression        = 0.5\n",
    "    img_input =   Input(shape=(input_shape))\n",
    "    def conv(x, out_filters, k_size):\n",
    "        return Conv2D(filters=out_filters,\n",
    "                      kernel_size=k_size,\n",
    "                      strides=(1,1),\n",
    "                      padding='same',\n",
    "                      kernel_initializer='he_normal',\n",
    "                      use_bias=False)(x)\n",
    "\n",
    "    def dense_layer(x):\n",
    "        return Dense(units=classes_num,\n",
    "                     activation='softmax',\n",
    "                     kernel_initializer='he_normal')(x)\n",
    "\n",
    "    def bn_relu(x):\n",
    "        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "    def bottleneck(x):\n",
    "        channels = growth_rate * 4\n",
    "        x = bn_relu(x)\n",
    "        x = conv(x, channels, (1,1))\n",
    "        x = bn_relu(x)\n",
    "        x = conv(x, growth_rate, (3,3))\n",
    "        return x\n",
    "\n",
    "    def single(x):\n",
    "        x = bn_relu(x)\n",
    "        x = conv(x, growth_rate, (3,3))\n",
    "        return x\n",
    "\n",
    "    def transition(x, inchannels):\n",
    "        outchannels = int(inchannels * compression)\n",
    "        x = bn_relu(x)\n",
    "        x = conv(x, outchannels, (1,1))\n",
    "        x = AveragePooling2D((2,2), strides=(2, 2))(x)\n",
    "        return x, outchannels\n",
    "\n",
    "    def dense_block(x,blocks,nchannels):\n",
    "        concat = x\n",
    "        for i in range(blocks):\n",
    "            x = bottleneck(concat)\n",
    "            concat = concatenate([x,concat], axis=-1)\n",
    "            nchannels += growth_rate\n",
    "        return concat, nchannels\n",
    "\n",
    "\n",
    "    nblocks = (depth - 4) // 6 \n",
    "    nchannels = growth_rate * 2\n",
    "\n",
    "\n",
    "    x = conv(img_input, nchannels, (3,3))\n",
    "    x, nchannels = dense_block(x,nblocks,nchannels)\n",
    "    x, nchannels = transition(x,nchannels)\n",
    "    x, nchannels = dense_block(x,nblocks,nchannels)\n",
    "    x, nchannels = transition(x,nchannels)\n",
    "    x, nchannels = dense_block(x,nblocks,nchannels)\n",
    "    x = bn_relu(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = dense_layer(x)\n",
    "    model = Model(img_input, x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    \n",
    "image_size=32    \n",
    "filters=256    \n",
    "patch_size=2\n",
    "kernel_size=5\n",
    "depth=8\n",
    "num_classes=Y_test.shape[1]\n",
    "\n",
    " \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    " \n",
    "\n",
    "kernel_size = 3\n",
    "filters = 32\n",
    "dropout = 0.3\n",
    " \n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    " \n",
    "\n",
    "def clsclb(input_shape,  classes=Y_test.shape[1], conv_dropout=0.1, ):\n",
    "    inputs = Input(shape=(input_shape))\n",
    "    filters = 64\n",
    "    y = Conv2D(filters=filters,kernel_size=kernel_size,strides=1,activation='relu',kernel_initializer='he_normal')(inputs)\n",
    "    y = MaxPooling2D()(y)\n",
    "    y = BatchNormalization(axis=-1)(y)\n",
    "    y = Conv2D(filters=filters,kernel_size=kernel_size,strides=1,activation='relu',kernel_initializer='he_normal')(y)\n",
    "    y = MaxPooling2D()(y)\n",
    "    y = BatchNormalization(axis=-1)(y)\n",
    "    y = Conv2D(filters=filters,kernel_size=kernel_size,strides=1,activation='relu',kernel_initializer='he_normal')(y)\n",
    "    y = BatchNormalization(axis=-1)(y)\n",
    "    y = Flatten()(y)\n",
    " \n",
    "    y = Dropout(dropout)(y)\n",
    "    y= Dense(512, activation='relu')(y)\n",
    "    y = BatchNormalization(axis=-1)(y)\n",
    "    y= Dense(64, activation='relu')(y)\n",
    "    y = BatchNormalization(axis=-1)(y)\n",
    "    y= Dense(10, activation='relu')(y)\n",
    "    outputs = Dense(classes, activation='softmax')(y)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    " \n",
    "    \n",
    "def plot_sample(lr, sr):\n",
    "    plt.figure(figsize=(4, 3))\n",
    "\n",
    "    images = [lr, sr]\n",
    "    titles = ['denoised', 'noisy']\n",
    "\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    \n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.callbacks import * \n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change here and select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 13:51:39.163085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.184495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.184618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.185075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-21 13:51:39.185636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.185738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.185823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.979922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.980062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.980154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 13:51:39.980226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15327 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "ms = 1\n",
    "\n",
    " \n",
    "if ms==1:\n",
    "    model    =      clsclb(input_shape)\n",
    "    fw       =      model.get_weights()\n",
    "    modelo   =      clsclb(input_shape) \n",
    "    modelc   =      clsclb(input_shape) \n",
    "    modele   =      clsclb(input_shape)\n",
    "    modelo.set_weights(fw) \n",
    "    modelc.set_weights(fw) \n",
    "    ud= 7\n",
    "    \n",
    "    \n",
    "    \n",
    "elif ms==2:\n",
    "    model  =  VGG16_BN(input_tensor=None, input_shape=x_train.shape[1:], classes=Y_test.shape[1], conv_dropout=0.1, dropout=0.3, activation='relu')\n",
    "    fw     =  model.get_weights()\n",
    "    modelo =  VGG16_BN(input_tensor=None, input_shape=x_train.shape[1:], classes=Y_test.shape[1], conv_dropout=0.1, dropout=0.3, activation='relu')\n",
    "    modelc =  VGG16_BN(input_tensor=None, input_shape=x_train.shape[1:], classes=Y_test.shape[1], conv_dropout=0.1, dropout=0.3, activation='relu')\n",
    "    modele =  VGG16_BN(input_tensor=None, input_shape=x_train.shape[1:], classes=Y_test.shape[1], conv_dropout=0.1, dropout=0.3, activation='relu')\n",
    "    modelo.set_weights(fw) \n",
    "    modelc.set_weights(fw) \n",
    "    ud= 53\n",
    "\n",
    "elif ms==3:\n",
    "    model  =  cir(input_shape,Y_test.shape[1])\n",
    "    fw     =  model.get_weights()\n",
    "    modelo =  cir(input_shape,Y_test.shape[1])\n",
    "    modelc =  cir(input_shape,Y_test.shape[1])\n",
    "    modele =  cir(input_shape,Y_test.shape[1])\n",
    "    modelo.set_weights(fw) \n",
    "    modelc.set_weights(fw) \n",
    "    ud= 49\n",
    "    \n",
    "elif ms==4:\n",
    "    model    =      ResNetForCIFAR10(num_classes , 'resnet56', input_shape )\n",
    "    fw       =      model.get_weights()\n",
    "    modelo   =      ResNetForCIFAR10(num_classes , 'resnet56', input_shape )\n",
    "    modelc   =      ResNetForCIFAR10(num_classes , 'resnet56', input_shape )\n",
    "    modele   =      ResNetForCIFAR10(num_classes , 'resnet56', input_shape )                       \n",
    "    modelo.set_weights(fw) \n",
    "    modelc.set_weights(fw) \n",
    "    ud= 109\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelc.set_weights(fw)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
    "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
    "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
    "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def get_box(lambda_value):\n",
    "    IMG_SIZE =32\n",
    "    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n",
    "\n",
    "    cut_w = IMG_SIZE * cut_rat  # rw\n",
    "    cut_w = tf.cast(cut_w, tf.int32)\n",
    "\n",
    "    cut_h = IMG_SIZE * cut_rat  # rh\n",
    "    cut_h = tf.cast(cut_h, tf.int32)\n",
    "\n",
    "    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n",
    "    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n",
    "\n",
    "    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n",
    "    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n",
    "    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n",
    "    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n",
    "\n",
    "    target_h = bby2 - boundaryy1\n",
    "    if target_h == 0:\n",
    "        target_h += 1\n",
    "\n",
    "    target_w = bbx2 - boundaryx1\n",
    "    if target_w == 0:\n",
    "        target_w += 1\n",
    "\n",
    "    return boundaryx1, boundaryy1, target_h, target_w\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def cutmix(train_ds_one, train_ds_two):\n",
    "    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n",
    "    IMG_SIZE = image1.shape[0]\n",
    "    alpha = [0.25]\n",
    "    beta = [0.25]\n",
    "\n",
    "    # Get a sample from the Beta distribution\n",
    "    lambda_value = sample_beta_distribution(1, alpha, beta)\n",
    "\n",
    "    # Define Lambda\n",
    "    lambda_value = lambda_value[0][0]\n",
    "\n",
    "    # Get the bounding box offsets, heights and widths\n",
    "    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n",
    "\n",
    "    # Get a patch from the second image (`image2`)\n",
    "    crop2 = tf.image.crop_to_bounding_box(\n",
    "        image2, boundaryy1, boundaryx1, target_h, target_w\n",
    "    )\n",
    "    # Pad the `image2` patch (`crop2`) with the same offset\n",
    "    image2 = tf.image.pad_to_bounding_box(\n",
    "        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
    "    )\n",
    "    # Get a patch from the first image (`image1`)\n",
    "    crop1 = tf.image.crop_to_bounding_box(\n",
    "        image1, boundaryy1, boundaryx1, target_h, target_w\n",
    "    )\n",
    "    # Pad the `image1` patch (`crop1`) with the same offset\n",
    "    img1 = tf.image.pad_to_bounding_box(\n",
    "        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
    "    )\n",
    "\n",
    "    # Modify the first image by subtracting the patch from `image1`\n",
    "    # (before applying the `image2` patch)\n",
    "    image1 = image1 - img1\n",
    "    # Add the modified `image1` and `image2`  together to get the CutMix image\n",
    "    image = image1 + image2\n",
    "\n",
    "    # Adjust Lambda in accordance to the pixel ration\n",
    "    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n",
    "    lambda_value = tf.cast(lambda_value, tf.float32)\n",
    "\n",
    "    # Combine the labels of both images\n",
    "    label = lambda_value * label1 + (1 - lambda_value) * label2\n",
    "    return image, label\n",
    "\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## augmentation function\n",
    "def uxdata(X_traina, y_traina, X_val, y_val, bzs):\n",
    " \n",
    " \n",
    "    train_ds_one =  tf.data.Dataset.from_tensor_slices((X_traina, y_traina)).shuffle(1024)\n",
    "    train_ds_two = tf.data.Dataset.from_tensor_slices((X_traina, y_traina)).shuffle(1024)   \n",
    "    train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))               \n",
    "     \n",
    "    train_dataset= (\n",
    "    train_ds.shuffle(1024).map(cutmix, num_parallel_calls=AUTO).batch(bzs).prefetch(AUTO))               \n",
    "                    \n",
    "                    \n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    val_dataset = val_dataset.batch(bzs)\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric   = tf.keras.metrics.CategoricalAccuracy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## add gradients/prarameters \n",
    "def agy(parameters, gradients ):\n",
    "    new_grads = []\n",
    "    for (params, grads) in zip(parameters, gradients):\n",
    " \n",
    "        ap = 5e-1\n",
    "        gt = (grads)  +(params)\n",
    "        new_grads.append( (gt)  )\n",
    "    return new_grads  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## add noise to the last layer\n",
    "\n",
    "def gnl(gradis):\n",
    "    cgrads = []\n",
    "    for i in range(len(gradis)):\n",
    "        grad = gradis[i]\n",
    "        if i== len(gradis)-1:\n",
    "            rv=  (tf.random.normal(grad.shape, mean=0.0, stddev=1e-1, dtype=tf.dtypes.float32))\n",
    "            grad = grad*rv\n",
    "            \n",
    "        cgrads.append(grad)\n",
    "    return cgrads\n",
    "\n",
    "\n",
    "\n",
    "## add noise to each layer\n",
    "\n",
    "def gna(gradis):\n",
    "    cgrads = []\n",
    "    for grad in gradis:\n",
    "        rank = len(grad.shape)\n",
    "        if rank > 1:\n",
    "            rv=  (tf.random.normal(grad.shape, mean=0.0, stddev=1e-4, dtype=tf.dtypes.float32))\n",
    "            grad = grad+rv\n",
    "        cgrads.append(grad)\n",
    "    return cgrads \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## add noise to salient layer\n",
    "\n",
    "\n",
    "\n",
    "def gnsn(gradis,yd):\n",
    "    cgrads = []\n",
    "    for i in range(len(gradis)):\n",
    "        grad = gradis[i]\n",
    "        if i== yd:\n",
    "            rv=  (tf.random.normal(grad.shape, mean=0.0, stddev=1e-1, dtype=tf.dtypes.float32))\n",
    "            grad = grad+rv\n",
    "            \n",
    "        cgrads.append(grad)\n",
    "    return cgrads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gradient centralization\n",
    " \n",
    "\n",
    "def gc(gradis):\n",
    "    cgrads = []\n",
    "    for grad in gradis:\n",
    "        rank = len(grad.shape)\n",
    "        if rank > 1:\n",
    "            grad -= tf.reduce_mean(grad, axis=list(range(rank-1)))\n",
    "        cgrads.append(grad)\n",
    "    return cgrads\n",
    "\n",
    "\n",
    "\n",
    "## adaptive gradient clipping\n",
    "\n",
    "\n",
    "def AGCO(parameters, gradients, clip_factor=0.01,\n",
    "                       eps=1e-3):\n",
    "    new_grads = []\n",
    "    for (params, grads) in zip(parameters, gradients):\n",
    "        p_norm = unitwise_norm(params)\n",
    "        max_norm = tf.math.maximum(p_norm, eps) * clip_factor\n",
    "        grad_norm = unitwise_norm(grads)\n",
    "        clipped_grad = grads * (max_norm / tf.math.maximum(grad_norm, 1e-6))\n",
    "        new_grad = tf.where(grad_norm < max_norm, grads, clipped_grad)\n",
    "        new_grads.append(new_grad)\n",
    "    return new_grads\n",
    "    \n",
    "def compute_norm(x, axis, keepdims):\n",
    "    return tf.math.reduce_sum( abs(x)**2 , axis=axis, keepdims=keepdims) ** .5\n",
    "\n",
    "def unitwise_norm(x):\n",
    "    if len(x.get_shape()) <= 1:  # Scalars and vectors\n",
    "        axis = None\n",
    "        keepdims = False\n",
    "    elif len(x.get_shape()) in [2, 3]:  # Linear layers of shape IO or multihead linear\n",
    "        axis = 0\n",
    "        keepdims = True\n",
    "    elif len(x.get_shape()) == 4:  # Conv kernels of shape HWIO\n",
    "        axis = [0, 1, 2,]\n",
    "        keepdims = True\n",
    "    else:\n",
    "        raise ValueError(f\"Got a parameter with shape not in [1, 2, 4]! {x}\")\n",
    "    return compute_norm(x, axis, keepdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld = tf.keras.losses.KLDivergence()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_setup_baseline(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    #model = tf.keras.models.experimental.SharpnessAwareMinimization(model, rho=0.05, num_batch_splits=None, name=None)\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_valuo =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_valuo , (model.trainable_weights)) \n",
    "            optimizer.apply_gradients(zip( (grams),   ( model.trainable_weights)   ))\n",
    " \n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_valuo, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt\n",
    "\n",
    "\n",
    "\n",
    "def train_setup_gc(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_valuo =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_valuo , (model.trainable_weights)) \n",
    "            optimizer.apply_gradients(zip( gc(grams),   ( model.trainable_weights)   ))\n",
    " \n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_valuo, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt\n",
    "\n",
    "\n",
    "\n",
    "def train_setup_agc(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_valuo =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_valuo , (model.trainable_weights)) \n",
    "            #grams = pgy(gna ( grams),grams)     \n",
    "            grams = (AGCO(model.trainable_weights, grams)) \n",
    "            \n",
    "            optimizer.apply_gradients(zip( (grams),   ( model.trainable_weights)   ))\n",
    " \n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_valuo, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt\n",
    "\n",
    "def train_setup_agc_gc(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_valuo =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_valuo , (model.trainable_weights)) \n",
    "            grams = pgy(gna ( grams),grams)     \n",
    "            #grams = gc(AGCO(model.trainable_weights, grams)) \n",
    "            \n",
    "            optimizer.apply_gradients(zip( (grams),   ( model.trainable_weights)   ))\n",
    " \n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_valuo, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt\n",
    "\n",
    "\n",
    "def train_setup_sam(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_valuo =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_valuo , (model.trainable_weights)) \n",
    "            \n",
    "            rho=0.05\n",
    "            eps=1e-12\n",
    "            e_ws = []\n",
    "                \n",
    "            grad_norm = tf.linalg.global_norm(grams)\n",
    "            ew_multiplier = rho / (grad_norm + eps)\n",
    "                \n",
    "            for i in range(len(model.trainable_weights)):\n",
    "                    \n",
    "                    e_w = tf.math.multiply(grams[i], ew_multiplier)\n",
    "                    \n",
    "                    model.trainable_weights[i].assign_add(e_w)\n",
    "                    e_ws.append(e_w)     \n",
    "                    \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "\n",
    "                    logit1 = model(xm, training=True)\n",
    "                    \n",
    "                    loss_valu1 =   loss_fn(ym,  (logit1)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_valu1 , (model.trainable_weights)) \n",
    "                \n",
    "            for i in range(len(model.trainable_weights)):\n",
    "                        \n",
    "                        model.trainable_weights[i].assign_sub(e_ws[i])\n",
    "\n",
    "            \n",
    "            \n",
    "            optimizer.apply_gradients(zip( (grams),   ( model.trainable_weights)   ))\n",
    " \n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_valuo, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAD- Teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_setup1(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_valuo =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_valuo , (model.trainable_weights)) \n",
    "            optimizer.apply_gradients(zip( (grams),   ( model.trainable_weights)   ))\n",
    "            \n",
    "            \n",
    "            #------------------------------Perturb paprameters and distill with auxiliary teacher\n",
    "            we1 = model.get_weights()\n",
    "            model.set_weights(gna(we1)) \n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                      xm = x_batch_train \n",
    "                \n",
    "                      ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                      logitse = model(xm, training=True)\n",
    "                      \n",
    "                      loss_value =   kld(logito,logitse)  \n",
    "    \n",
    "            model.set_weights( (we1)) \n",
    "        \n",
    "            gramk = tape.gradient(loss_value , (model.trainable_weights)) \n",
    " \n",
    "            optimizer.apply_gradients(zip( agy(grams,gramk),   ( model.trainable_weights)   ))\n",
    "    \n",
    "            #----------------------------- final update done ---------------------\n",
    "    \n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_value, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_setup2(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_value =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_value , (model.trainable_weights)) \n",
    "            optimizer.apply_gradients(zip( (grams),   ( model.trainable_weights)   ))\n",
    "            \n",
    "            #------------------------------Perturb paprameters and distill with auxiliary teacher\n",
    "            \n",
    "            we1 = model.get_weights()\n",
    "            model.set_weights(gnl(we1)) # last layer noise\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    din =       (x_batch_train )   \n",
    "                      \n",
    "                    logitse = model(din, training=True)\n",
    "                    \n",
    "\n",
    "                    model.set_weights( gnsn(we1,ud)) # salient layer noise, ud is salient layer id\n",
    "                    \n",
    "                    logitse1 = model(din, training=True)\n",
    "                    \n",
    "                    loss_value1 =   kld(logitse,   logito)  \n",
    "                    loss_value2 =   kld(logitse1,  logito)    \n",
    "    \n",
    "            model.set_weights( (we1)) \n",
    "        \n",
    "            gramk1 = tape.gradient(loss_value1 , (model.trainable_weights)) \n",
    "            gramk2 = tape.gradient(loss_value2 , (model.trainable_weights)) \n",
    "             \n",
    "            gramk = agy(gramk2,gramk1) ## add grads\n",
    " \n",
    "            optimizer.apply_gradients(zip( agy(grams,gramk) ,   ( model.trainable_weights)   ))\n",
    "            #----------------------------- final update done ---------------------\n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_value, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt\n",
    "\n",
    "def train_setup3(tx_data,vl_data,model,epc,tid):\n",
    "    \n",
    "    train_dataset = tx_data\n",
    "    val_dataset = vl_data\n",
    "    \n",
    "    epochs = epc\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)  \n",
    "    kld = tf.keras.losses.KLDivergence()\n",
    "    \n",
    "    \n",
    "    fwo = model.get_weights()\n",
    "    \n",
    "    auwt = list()\n",
    "    auwt.append(fwo)\n",
    "    \n",
    "    val_sc = list()\n",
    "    tr_ac_hi=[]\n",
    "    val_ac_hi=[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        \n",
    "        lr_schedule = keras.optimizers.schedules.ExponentialDecay(1e-4, decay_steps=1000, decay_rate=0.9) \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        \n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    \n",
    "                    \n",
    "                    xm = x_batch_train   \n",
    "                    \n",
    "                    ym = y_batch_train\n",
    "                    \n",
    "                    \n",
    "                    logito = model(xm, training=True)\n",
    "                    loss_value =   loss_fn(ym,  (logito)) \n",
    "                    \n",
    "            grams = tape.gradient(loss_value , (model.trainable_weights)) \n",
    "            optimizer.apply_gradients(zip( (grams),   ( model.trainable_weights)   ))\n",
    "            \n",
    "            \n",
    "            \n",
    "            #------------------------------Perturb paprameters and distill with auxiliary teacher\n",
    "            we1 = model.get_weights()\n",
    "            grans = gna(grams)\n",
    "            \n",
    "            for i in range(len(model.trainable_weights)):\n",
    "                    e_w = (1e-3*grans[i])\n",
    "                    model.trainable_weights[i].assign_add(e_w)\n",
    "            \n",
    "         \n",
    "            \n",
    "            with tf.GradientTape(persistent = True) as tape:\n",
    "                    din =       (x_batch_train )   \n",
    "                      \n",
    "                    logitse = model(din, training=True)\n",
    " \n",
    "                    loss_value1 =   kld(logitse,   logito)  \n",
    "                    \n",
    "    \n",
    "            model.set_weights( (we1)) \n",
    "        \n",
    "            gramk  = tape.gradient(loss_value1 , (model.trainable_weights)) \n",
    " \n",
    " \n",
    "            optimizer.apply_gradients(zip( agy(grams,gramk),   ( model.trainable_weights)   ))\n",
    "            \n",
    "            #----------------------------- final update done ---------------------\n",
    "            \n",
    "    \n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(y_batch_train, logito)\n",
    "\n",
    "            # Display metrics at the end of each epoch.\n",
    "            train_acc = train_acc_metric.result()\n",
    "            \n",
    "            \n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            val_logits = model(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_acc_metric.update_state(y_batch_val, val_logits)\n",
    "        \n",
    "        \n",
    "        val_acc = val_acc_metric.result()\n",
    "            \n",
    "        val_loss_value = loss_fn(y_batch_val, val_logits)\n",
    "    \n",
    "        tr_ac_hi.append(\"%.3f\" % float(train_acc_metric.result()))\n",
    "        val_ac_hi.append(\"%.3f\" % float(val_acc_metric.result()))\n",
    "        \n",
    "        template = 'ETA: {} - epoch: {} loss: {}  acc: {} val loss: {} val acc: {}\\n'\n",
    "        print(template.format(round((time.time() -  start_time)/60, 2), epoch + 1,\n",
    "                     \"%.3f\" %  loss_value, \"%.3f\" % float(train_acc_metric.result()),\n",
    "                              \"%.3f\" % val_loss_value, \"%.3f\" % float(val_acc_metric.result())))\n",
    "    \n",
    "        vsd = val_loss_value\n",
    "        val_sc.append(vsd)\n",
    "        \n",
    "        \n",
    "        val_acc_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    " \n",
    "        print(\"At setup \"+ str(tid)+ \" taken Time is : %.2fs\" % (time.time() - start_time)) \n",
    "           \n",
    "        if epoch>=3 and vsd<np.sort(val_sc)[1]:\n",
    "            auwt[0] = model.get_weights()\n",
    "            print('Best weight saved')\n",
    " \n",
    "\n",
    "\n",
    "    return  auwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change here..........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfg = 50000   #[full data or reduce as you like]\n",
    "\n",
    "bzs = 4096//2  ## 8 or 2 for 512 or 2048\n",
    "train_dax, val_dx = uxdata(x_train[0:tfg] , y_train[0:tfg] , x_val, y_val, bzs) ## uxdata for cutmix\n",
    "\n",
    "hw = list()\n",
    "\n",
    "echp = 20  # 200 for 512 or 370 for 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 13:51:45.009940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:426] Loaded cuDNN version 8302\n",
      "2022-09-21 13:51:46.461790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 0.12 - epoch: 1 loss: 2.420  acc: 0.138 val loss: 2.295 val acc: 0.122\n",
      "\n",
      "At setup 0 taken Time is : 7.15s\n",
      "\n",
      "Start of epoch 1\n",
      "ETA: 0.03 - epoch: 2 loss: 2.330  acc: 0.180 val loss: 2.252 val acc: 0.176\n",
      "\n",
      "At setup 0 taken Time is : 1.89s\n",
      "\n",
      "Start of epoch 2\n",
      "ETA: 0.03 - epoch: 3 loss: 2.226  acc: 0.203 val loss: 2.196 val acc: 0.206\n",
      "\n",
      "At setup 0 taken Time is : 1.87s\n",
      "\n",
      "Start of epoch 3\n",
      "ETA: 0.03 - epoch: 4 loss: 2.152  acc: 0.230 val loss: 2.168 val acc: 0.212\n",
      "\n",
      "At setup 0 taken Time is : 1.83s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 4\n",
      "ETA: 0.03 - epoch: 5 loss: 2.095  acc: 0.254 val loss: 2.124 val acc: 0.206\n",
      "\n",
      "At setup 0 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 5\n",
      "ETA: 0.03 - epoch: 6 loss: 2.082  acc: 0.271 val loss: 2.087 val acc: 0.213\n",
      "\n",
      "At setup 0 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 6\n",
      "ETA: 0.03 - epoch: 7 loss: 2.046  acc: 0.288 val loss: 2.039 val acc: 0.240\n",
      "\n",
      "At setup 0 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 7\n",
      "ETA: 0.03 - epoch: 8 loss: 2.032  acc: 0.300 val loss: 2.022 val acc: 0.248\n",
      "\n",
      "At setup 0 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 8\n",
      "ETA: 0.03 - epoch: 9 loss: 2.010  acc: 0.317 val loss: 1.991 val acc: 0.272\n",
      "\n",
      "At setup 0 taken Time is : 1.93s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 9\n",
      "ETA: 0.03 - epoch: 10 loss: 2.017  acc: 0.325 val loss: 1.944 val acc: 0.304\n",
      "\n",
      "At setup 0 taken Time is : 1.93s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 10\n",
      "ETA: 0.03 - epoch: 11 loss: 1.954  acc: 0.332 val loss: 1.920 val acc: 0.321\n",
      "\n",
      "At setup 0 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 11\n",
      "ETA: 0.03 - epoch: 12 loss: 1.939  acc: 0.346 val loss: 1.879 val acc: 0.340\n",
      "\n",
      "At setup 0 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 12\n",
      "ETA: 0.03 - epoch: 13 loss: 1.966  acc: 0.350 val loss: 1.829 val acc: 0.358\n",
      "\n",
      "At setup 0 taken Time is : 1.90s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 13\n",
      "ETA: 0.03 - epoch: 14 loss: 1.945  acc: 0.357 val loss: 1.796 val acc: 0.369\n",
      "\n",
      "At setup 0 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 14\n",
      "ETA: 0.03 - epoch: 15 loss: 1.891  acc: 0.362 val loss: 1.760 val acc: 0.385\n",
      "\n",
      "At setup 0 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 15\n",
      "ETA: 0.03 - epoch: 16 loss: 1.903  acc: 0.366 val loss: 1.689 val acc: 0.408\n",
      "\n",
      "At setup 0 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 16\n",
      "ETA: 0.03 - epoch: 17 loss: 1.864  acc: 0.377 val loss: 1.656 val acc: 0.419\n",
      "\n",
      "At setup 0 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 17\n",
      "ETA: 0.03 - epoch: 18 loss: 1.872  acc: 0.380 val loss: 1.602 val acc: 0.427\n",
      "\n",
      "At setup 0 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 18\n",
      "ETA: 0.03 - epoch: 19 loss: 1.848  acc: 0.391 val loss: 1.559 val acc: 0.441\n",
      "\n",
      "At setup 0 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 19\n",
      "ETA: 0.03 - epoch: 20 loss: 1.886  acc: 0.396 val loss: 1.522 val acc: 0.452\n",
      "\n",
      "At setup 0 taken Time is : 1.93s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 0\n",
      "ETA: 0.03 - epoch: 1 loss: 2.423  acc: 0.139 val loss: 2.288 val acc: 0.130\n",
      "\n",
      "At setup 1 taken Time is : 1.90s\n",
      "\n",
      "Start of epoch 1\n",
      "ETA: 0.03 - epoch: 2 loss: 2.299  acc: 0.175 val loss: 2.254 val acc: 0.187\n",
      "\n",
      "At setup 1 taken Time is : 1.90s\n",
      "\n",
      "Start of epoch 2\n",
      "ETA: 0.03 - epoch: 3 loss: 2.273  acc: 0.202 val loss: 2.203 val acc: 0.196\n",
      "\n",
      "At setup 1 taken Time is : 1.93s\n",
      "\n",
      "Start of epoch 3\n",
      "ETA: 0.03 - epoch: 4 loss: 2.165  acc: 0.222 val loss: 2.157 val acc: 0.212\n",
      "\n",
      "At setup 1 taken Time is : 1.91s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 4\n",
      "ETA: 0.03 - epoch: 5 loss: 2.098  acc: 0.246 val loss: 2.117 val acc: 0.224\n",
      "\n",
      "At setup 1 taken Time is : 1.97s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 5\n",
      "ETA: 0.03 - epoch: 6 loss: 2.091  acc: 0.266 val loss: 2.083 val acc: 0.223\n",
      "\n",
      "At setup 1 taken Time is : 1.93s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 6\n",
      "ETA: 0.03 - epoch: 7 loss: 2.109  acc: 0.282 val loss: 2.043 val acc: 0.243\n",
      "\n",
      "At setup 1 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 7\n",
      "ETA: 0.03 - epoch: 8 loss: 2.049  acc: 0.293 val loss: 2.019 val acc: 0.266\n",
      "\n",
      "At setup 1 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 8\n",
      "ETA: 0.03 - epoch: 9 loss: 2.044  acc: 0.306 val loss: 1.969 val acc: 0.297\n",
      "\n",
      "At setup 1 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 9\n",
      "ETA: 0.03 - epoch: 10 loss: 1.993  acc: 0.315 val loss: 1.939 val acc: 0.316\n",
      "\n",
      "At setup 1 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 10\n",
      "ETA: 0.03 - epoch: 11 loss: 1.994  acc: 0.322 val loss: 1.898 val acc: 0.330\n",
      "\n",
      "At setup 1 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 11\n",
      "ETA: 0.03 - epoch: 12 loss: 1.975  acc: 0.332 val loss: 1.875 val acc: 0.342\n",
      "\n",
      "At setup 1 taken Time is : 1.93s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 12\n",
      "ETA: 0.03 - epoch: 13 loss: 2.028  acc: 0.340 val loss: 1.826 val acc: 0.366\n",
      "\n",
      "At setup 1 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 13\n",
      "ETA: 0.03 - epoch: 14 loss: 1.939  acc: 0.345 val loss: 1.780 val acc: 0.378\n",
      "\n",
      "At setup 1 taken Time is : 1.91s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 14\n",
      "ETA: 0.03 - epoch: 15 loss: 1.916  acc: 0.351 val loss: 1.753 val acc: 0.390\n",
      "\n",
      "At setup 1 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 15\n",
      "ETA: 0.03 - epoch: 16 loss: 1.910  acc: 0.359 val loss: 1.712 val acc: 0.401\n",
      "\n",
      "At setup 1 taken Time is : 1.93s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 16\n",
      "ETA: 0.03 - epoch: 17 loss: 1.902  acc: 0.363 val loss: 1.675 val acc: 0.414\n",
      "\n",
      "At setup 1 taken Time is : 1.93s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 17\n",
      "ETA: 0.03 - epoch: 18 loss: 1.881  acc: 0.365 val loss: 1.644 val acc: 0.420\n",
      "\n",
      "At setup 1 taken Time is : 1.95s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 18\n",
      "ETA: 0.03 - epoch: 19 loss: 1.870  acc: 0.369 val loss: 1.605 val acc: 0.436\n",
      "\n",
      "At setup 1 taken Time is : 1.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 19\n",
      "ETA: 0.03 - epoch: 20 loss: 1.888  acc: 0.375 val loss: 1.577 val acc: 0.446\n",
      "\n",
      "At setup 1 taken Time is : 1.91s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 0\n",
      "ETA: 0.03 - epoch: 1 loss: 2.451  acc: 0.141 val loss: 2.298 val acc: 0.119\n",
      "\n",
      "At setup 2 taken Time is : 2.09s\n",
      "\n",
      "Start of epoch 1\n",
      "ETA: 0.03 - epoch: 2 loss: 2.304  acc: 0.177 val loss: 2.251 val acc: 0.182\n",
      "\n",
      "At setup 2 taken Time is : 2.04s\n",
      "\n",
      "Start of epoch 2\n",
      "ETA: 0.03 - epoch: 3 loss: 2.225  acc: 0.205 val loss: 2.211 val acc: 0.204\n",
      "\n",
      "At setup 2 taken Time is : 2.03s\n",
      "\n",
      "Start of epoch 3\n",
      "ETA: 0.03 - epoch: 4 loss: 2.178  acc: 0.227 val loss: 2.161 val acc: 0.219\n",
      "\n",
      "At setup 2 taken Time is : 2.02s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 4\n",
      "ETA: 0.03 - epoch: 5 loss: 2.087  acc: 0.256 val loss: 2.105 val acc: 0.221\n",
      "\n",
      "At setup 2 taken Time is : 2.03s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 5\n",
      "ETA: 0.03 - epoch: 6 loss: 2.085  acc: 0.278 val loss: 2.066 val acc: 0.227\n",
      "\n",
      "At setup 2 taken Time is : 2.04s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 6\n",
      "ETA: 0.03 - epoch: 7 loss: 2.054  acc: 0.291 val loss: 2.034 val acc: 0.252\n",
      "\n",
      "At setup 2 taken Time is : 2.02s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 7\n",
      "ETA: 0.03 - epoch: 8 loss: 1.974  acc: 0.307 val loss: 2.021 val acc: 0.261\n",
      "\n",
      "At setup 2 taken Time is : 2.03s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 8\n",
      "ETA: 0.03 - epoch: 9 loss: 2.004  acc: 0.318 val loss: 1.988 val acc: 0.279\n",
      "\n",
      "At setup 2 taken Time is : 2.04s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 9\n",
      "ETA: 0.03 - epoch: 10 loss: 1.988  acc: 0.325 val loss: 1.967 val acc: 0.298\n",
      "\n",
      "At setup 2 taken Time is : 2.03s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 10\n",
      "ETA: 0.03 - epoch: 11 loss: 1.948  acc: 0.337 val loss: 1.922 val acc: 0.317\n",
      "\n",
      "At setup 2 taken Time is : 2.02s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 11\n",
      "ETA: 0.03 - epoch: 12 loss: 1.926  acc: 0.344 val loss: 1.865 val acc: 0.343\n",
      "\n",
      "At setup 2 taken Time is : 2.04s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 12\n",
      "ETA: 0.03 - epoch: 13 loss: 1.898  acc: 0.349 val loss: 1.824 val acc: 0.358\n",
      "\n",
      "At setup 2 taken Time is : 2.02s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 13\n",
      "ETA: 0.03 - epoch: 14 loss: 1.932  acc: 0.352 val loss: 1.778 val acc: 0.374\n",
      "\n",
      "At setup 2 taken Time is : 2.03s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 14\n",
      "ETA: 0.03 - epoch: 15 loss: 1.911  acc: 0.361 val loss: 1.726 val acc: 0.394\n",
      "\n",
      "At setup 2 taken Time is : 2.02s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 15\n",
      "ETA: 0.03 - epoch: 16 loss: 1.878  acc: 0.370 val loss: 1.690 val acc: 0.412\n",
      "\n",
      "At setup 2 taken Time is : 2.01s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 16\n",
      "ETA: 0.03 - epoch: 17 loss: 1.858  acc: 0.376 val loss: 1.648 val acc: 0.421\n",
      "\n",
      "At setup 2 taken Time is : 2.04s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 0.03 - epoch: 18 loss: 1.880  acc: 0.382 val loss: 1.598 val acc: 0.442\n",
      "\n",
      "At setup 2 taken Time is : 2.04s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 18\n",
      "ETA: 0.03 - epoch: 19 loss: 1.834  acc: 0.390 val loss: 1.552 val acc: 0.454\n",
      "\n",
      "At setup 2 taken Time is : 2.03s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 19\n",
      "ETA: 0.03 - epoch: 20 loss: 1.824  acc: 0.396 val loss: 1.526 val acc: 0.460\n",
      "\n",
      "At setup 2 taken Time is : 1.98s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 0\n",
      "ETA: 0.06 - epoch: 1 loss: 2.405  acc: 0.137 val loss: 2.280 val acc: 0.148\n",
      "\n",
      "At setup 3 taken Time is : 3.39s\n",
      "\n",
      "Start of epoch 1\n",
      "ETA: 0.06 - epoch: 2 loss: 2.292  acc: 0.176 val loss: 2.229 val acc: 0.190\n",
      "\n",
      "At setup 3 taken Time is : 3.40s\n",
      "\n",
      "Start of epoch 2\n",
      "ETA: 0.06 - epoch: 3 loss: 2.192  acc: 0.204 val loss: 2.191 val acc: 0.197\n",
      "\n",
      "At setup 3 taken Time is : 3.37s\n",
      "\n",
      "Start of epoch 3\n",
      "ETA: 0.06 - epoch: 4 loss: 2.201  acc: 0.229 val loss: 2.131 val acc: 0.205\n",
      "\n",
      "At setup 3 taken Time is : 3.37s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 4\n",
      "ETA: 0.06 - epoch: 5 loss: 2.096  acc: 0.250 val loss: 2.069 val acc: 0.221\n",
      "\n",
      "At setup 3 taken Time is : 3.38s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 5\n",
      "ETA: 0.06 - epoch: 6 loss: 2.131  acc: 0.269 val loss: 1.991 val acc: 0.269\n",
      "\n",
      "At setup 3 taken Time is : 3.36s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 6\n",
      "ETA: 0.06 - epoch: 7 loss: 2.068  acc: 0.286 val loss: 1.931 val acc: 0.308\n",
      "\n",
      "At setup 3 taken Time is : 3.39s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 7\n",
      "ETA: 0.06 - epoch: 8 loss: 2.096  acc: 0.298 val loss: 1.860 val acc: 0.341\n",
      "\n",
      "At setup 3 taken Time is : 3.41s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 8\n",
      "ETA: 0.06 - epoch: 9 loss: 2.018  acc: 0.312 val loss: 1.796 val acc: 0.375\n",
      "\n",
      "At setup 3 taken Time is : 3.36s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 9\n",
      "ETA: 0.06 - epoch: 10 loss: 1.995  acc: 0.325 val loss: 1.742 val acc: 0.387\n",
      "\n",
      "At setup 3 taken Time is : 3.39s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 10\n",
      "ETA: 0.06 - epoch: 11 loss: 1.984  acc: 0.332 val loss: 1.705 val acc: 0.405\n",
      "\n",
      "At setup 3 taken Time is : 3.39s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 11\n",
      "ETA: 0.06 - epoch: 12 loss: 1.904  acc: 0.339 val loss: 1.662 val acc: 0.417\n",
      "\n",
      "At setup 3 taken Time is : 3.37s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 12\n",
      "ETA: 0.06 - epoch: 13 loss: 1.925  acc: 0.344 val loss: 1.630 val acc: 0.429\n",
      "\n",
      "At setup 3 taken Time is : 3.38s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 13\n",
      "ETA: 0.06 - epoch: 14 loss: 1.889  acc: 0.355 val loss: 1.615 val acc: 0.440\n",
      "\n",
      "At setup 3 taken Time is : 3.41s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 14\n",
      "ETA: 0.06 - epoch: 15 loss: 1.906  acc: 0.358 val loss: 1.586 val acc: 0.448\n",
      "\n",
      "At setup 3 taken Time is : 3.33s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 15\n",
      "ETA: 0.06 - epoch: 16 loss: 1.881  acc: 0.366 val loss: 1.564 val acc: 0.461\n",
      "\n",
      "At setup 3 taken Time is : 3.39s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 16\n",
      "ETA: 0.06 - epoch: 17 loss: 1.909  acc: 0.372 val loss: 1.547 val acc: 0.463\n",
      "\n",
      "At setup 3 taken Time is : 3.40s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 17\n",
      "ETA: 0.06 - epoch: 18 loss: 1.851  acc: 0.374 val loss: 1.529 val acc: 0.464\n",
      "\n",
      "At setup 3 taken Time is : 3.36s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 18\n",
      "ETA: 0.06 - epoch: 19 loss: 1.808  acc: 0.383 val loss: 1.509 val acc: 0.472\n",
      "\n",
      "At setup 3 taken Time is : 3.37s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 19\n",
      "ETA: 0.05 - epoch: 20 loss: 1.870  acc: 0.389 val loss: 1.493 val acc: 0.477\n",
      "\n",
      "At setup 3 taken Time is : 3.27s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 0\n",
      "ETA: 0.08 - epoch: 1 loss: 2.311  acc: 0.156 val loss: 2.290 val acc: 0.129\n",
      "\n",
      "At setup 4 taken Time is : 4.98s\n",
      "\n",
      "Start of epoch 1\n",
      "ETA: 0.08 - epoch: 2 loss: 2.183  acc: 0.212 val loss: 2.225 val acc: 0.203\n",
      "\n",
      "At setup 4 taken Time is : 4.94s\n",
      "\n",
      "Start of epoch 2\n",
      "ETA: 0.08 - epoch: 3 loss: 2.092  acc: 0.255 val loss: 2.145 val acc: 0.227\n",
      "\n",
      "At setup 4 taken Time is : 4.77s\n",
      "\n",
      "Start of epoch 3\n",
      "ETA: 0.08 - epoch: 4 loss: 2.008  acc: 0.291 val loss: 2.086 val acc: 0.227\n",
      "\n",
      "At setup 4 taken Time is : 4.85s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 4\n",
      "ETA: 0.08 - epoch: 5 loss: 1.966  acc: 0.316 val loss: 2.075 val acc: 0.226\n",
      "\n",
      "At setup 4 taken Time is : 4.87s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 5\n",
      "ETA: 0.08 - epoch: 6 loss: 1.944  acc: 0.334 val loss: 2.083 val acc: 0.237\n",
      "\n",
      "At setup 4 taken Time is : 4.84s\n",
      "\n",
      "Start of epoch 6\n",
      "ETA: 0.08 - epoch: 7 loss: 1.950  acc: 0.350 val loss: 2.092 val acc: 0.237\n",
      "\n",
      "At setup 4 taken Time is : 4.85s\n",
      "\n",
      "Start of epoch 7\n",
      "ETA: 0.08 - epoch: 8 loss: 1.887  acc: 0.361 val loss: 2.068 val acc: 0.249\n",
      "\n",
      "At setup 4 taken Time is : 4.77s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 8\n",
      "ETA: 0.08 - epoch: 9 loss: 1.911  acc: 0.376 val loss: 2.016 val acc: 0.282\n",
      "\n",
      "At setup 4 taken Time is : 4.96s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 9\n",
      "ETA: 0.08 - epoch: 10 loss: 1.816  acc: 0.388 val loss: 1.973 val acc: 0.303\n",
      "\n",
      "At setup 4 taken Time is : 4.86s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 10\n",
      "ETA: 0.08 - epoch: 11 loss: 1.788  acc: 0.402 val loss: 1.884 val acc: 0.329\n",
      "\n",
      "At setup 4 taken Time is : 4.64s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 11\n",
      "ETA: 0.08 - epoch: 12 loss: 1.798  acc: 0.410 val loss: 1.819 val acc: 0.360\n",
      "\n",
      "At setup 4 taken Time is : 4.86s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 12\n",
      "ETA: 0.08 - epoch: 13 loss: 1.796  acc: 0.423 val loss: 1.755 val acc: 0.383\n",
      "\n",
      "At setup 4 taken Time is : 4.88s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 13\n",
      "ETA: 0.08 - epoch: 14 loss: 1.820  acc: 0.431 val loss: 1.699 val acc: 0.400\n",
      "\n",
      "At setup 4 taken Time is : 4.87s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 14\n",
      "ETA: 0.08 - epoch: 15 loss: 1.710  acc: 0.439 val loss: 1.613 val acc: 0.432\n",
      "\n",
      "At setup 4 taken Time is : 4.74s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 15\n",
      "ETA: 0.08 - epoch: 16 loss: 1.722  acc: 0.448 val loss: 1.532 val acc: 0.458\n",
      "\n",
      "At setup 4 taken Time is : 4.75s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 16\n",
      "ETA: 0.08 - epoch: 17 loss: 1.698  acc: 0.454 val loss: 1.454 val acc: 0.483\n",
      "\n",
      "At setup 4 taken Time is : 4.97s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 17\n",
      "ETA: 0.08 - epoch: 18 loss: 1.671  acc: 0.462 val loss: 1.402 val acc: 0.515\n",
      "\n",
      "At setup 4 taken Time is : 4.87s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 18\n",
      "ETA: 0.08 - epoch: 19 loss: 1.680  acc: 0.469 val loss: 1.335 val acc: 0.543\n",
      "\n",
      "At setup 4 taken Time is : 4.94s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 19\n",
      "ETA: 0.08 - epoch: 20 loss: 1.708  acc: 0.477 val loss: 1.284 val acc: 0.567\n",
      "\n",
      "At setup 4 taken Time is : 4.87s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 0\n",
      "ETA: 0.15 - epoch: 1 loss: 2.322  acc: 0.155 val loss: 2.279 val acc: 0.133\n",
      "\n",
      "At setup 5 taken Time is : 8.90s\n",
      "\n",
      "Start of epoch 1\n",
      "ETA: 0.15 - epoch: 2 loss: 2.152  acc: 0.208 val loss: 2.241 val acc: 0.201\n",
      "\n",
      "At setup 5 taken Time is : 9.16s\n",
      "\n",
      "Start of epoch 2\n",
      "ETA: 0.15 - epoch: 3 loss: 2.074  acc: 0.250 val loss: 2.155 val acc: 0.228\n",
      "\n",
      "At setup 5 taken Time is : 9.10s\n",
      "\n",
      "Start of epoch 3\n",
      "ETA: 0.15 - epoch: 4 loss: 2.023  acc: 0.289 val loss: 2.088 val acc: 0.222\n",
      "\n",
      "At setup 5 taken Time is : 9.19s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 4\n",
      "ETA: 0.15 - epoch: 5 loss: 2.025  acc: 0.313 val loss: 2.065 val acc: 0.219\n",
      "\n",
      "At setup 5 taken Time is : 9.17s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 5\n",
      "ETA: 0.15 - epoch: 6 loss: 1.980  acc: 0.332 val loss: 2.046 val acc: 0.238\n",
      "\n",
      "At setup 5 taken Time is : 9.25s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 6\n",
      "ETA: 0.14 - epoch: 7 loss: 1.950  acc: 0.348 val loss: 2.061 val acc: 0.243\n",
      "\n",
      "At setup 5 taken Time is : 8.40s\n",
      "\n",
      "Start of epoch 7\n",
      "ETA: 0.15 - epoch: 8 loss: 1.888  acc: 0.361 val loss: 2.037 val acc: 0.261\n",
      "\n",
      "At setup 5 taken Time is : 8.76s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 8\n",
      "ETA: 0.14 - epoch: 9 loss: 1.894  acc: 0.372 val loss: 1.988 val acc: 0.288\n",
      "\n",
      "At setup 5 taken Time is : 8.64s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 9\n",
      "ETA: 0.14 - epoch: 10 loss: 1.848  acc: 0.389 val loss: 2.001 val acc: 0.287\n",
      "\n",
      "At setup 5 taken Time is : 8.65s\n",
      "\n",
      "Start of epoch 10\n",
      "ETA: 0.14 - epoch: 11 loss: 1.853  acc: 0.400 val loss: 1.895 val acc: 0.328\n",
      "\n",
      "At setup 5 taken Time is : 8.67s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 11\n",
      "ETA: 0.14 - epoch: 12 loss: 1.762  acc: 0.414 val loss: 1.816 val acc: 0.357\n",
      "\n",
      "At setup 5 taken Time is : 8.64s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 12\n",
      "ETA: 0.14 - epoch: 13 loss: 1.783  acc: 0.422 val loss: 1.756 val acc: 0.383\n",
      "\n",
      "At setup 5 taken Time is : 8.66s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 13\n",
      "ETA: 0.14 - epoch: 14 loss: 1.817  acc: 0.429 val loss: 1.676 val acc: 0.418\n",
      "\n",
      "At setup 5 taken Time is : 8.67s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 14\n",
      "ETA: 0.14 - epoch: 15 loss: 1.758  acc: 0.437 val loss: 1.594 val acc: 0.445\n",
      "\n",
      "At setup 5 taken Time is : 8.65s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETA: 0.14 - epoch: 16 loss: 1.737  acc: 0.446 val loss: 1.527 val acc: 0.467\n",
      "\n",
      "At setup 5 taken Time is : 8.66s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 16\n",
      "ETA: 0.14 - epoch: 17 loss: 1.722  acc: 0.451 val loss: 1.431 val acc: 0.499\n",
      "\n",
      "At setup 5 taken Time is : 8.67s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 17\n",
      "ETA: 0.15 - epoch: 18 loss: 1.710  acc: 0.462 val loss: 1.374 val acc: 0.521\n",
      "\n",
      "At setup 5 taken Time is : 9.00s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 18\n",
      "ETA: 0.15 - epoch: 19 loss: 1.665  acc: 0.468 val loss: 1.322 val acc: 0.551\n",
      "\n",
      "At setup 5 taken Time is : 9.17s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 19\n",
      "ETA: 0.15 - epoch: 20 loss: 1.663  acc: 0.474 val loss: 1.286 val acc: 0.562\n",
      "\n",
      "At setup 5 taken Time is : 9.23s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 0\n",
      "ETA: 0.11 - epoch: 1 loss: 0.124  acc: 0.156 val loss: 2.302 val acc: 0.120\n",
      "\n",
      "At setup 6 taken Time is : 6.70s\n",
      "\n",
      "Start of epoch 1\n",
      "ETA: 0.11 - epoch: 2 loss: 0.079  acc: 0.211 val loss: 2.245 val acc: 0.176\n",
      "\n",
      "At setup 6 taken Time is : 6.77s\n",
      "\n",
      "Start of epoch 2\n",
      "ETA: 0.11 - epoch: 3 loss: 0.055  acc: 0.256 val loss: 2.157 val acc: 0.228\n",
      "\n",
      "At setup 6 taken Time is : 6.77s\n",
      "\n",
      "Start of epoch 3\n",
      "ETA: 0.11 - epoch: 4 loss: 0.050  acc: 0.288 val loss: 2.104 val acc: 0.220\n",
      "\n",
      "At setup 6 taken Time is : 6.73s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 4\n",
      "ETA: 0.1 - epoch: 5 loss: 0.044  acc: 0.311 val loss: 2.065 val acc: 0.240\n",
      "\n",
      "At setup 6 taken Time is : 6.27s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 5\n",
      "ETA: 0.11 - epoch: 6 loss: 0.042  acc: 0.333 val loss: 2.061 val acc: 0.243\n",
      "\n",
      "At setup 6 taken Time is : 6.63s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 6\n",
      "ETA: 0.11 - epoch: 7 loss: 0.045  acc: 0.346 val loss: 2.073 val acc: 0.241\n",
      "\n",
      "At setup 6 taken Time is : 6.63s\n",
      "\n",
      "Start of epoch 7\n",
      "ETA: 0.11 - epoch: 8 loss: 0.043  acc: 0.360 val loss: 2.037 val acc: 0.267\n",
      "\n",
      "At setup 6 taken Time is : 6.62s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 8\n",
      "ETA: 0.11 - epoch: 9 loss: 0.042  acc: 0.369 val loss: 2.005 val acc: 0.282\n",
      "\n",
      "At setup 6 taken Time is : 6.64s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 9\n",
      "ETA: 0.11 - epoch: 10 loss: 0.041  acc: 0.386 val loss: 1.993 val acc: 0.296\n",
      "\n",
      "At setup 6 taken Time is : 6.73s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 10\n",
      "ETA: 0.11 - epoch: 11 loss: 0.045  acc: 0.397 val loss: 1.947 val acc: 0.314\n",
      "\n",
      "At setup 6 taken Time is : 6.63s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 11\n",
      "ETA: 0.11 - epoch: 12 loss: 0.043  acc: 0.410 val loss: 1.883 val acc: 0.339\n",
      "\n",
      "At setup 6 taken Time is : 6.63s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 12\n",
      "ETA: 0.11 - epoch: 13 loss: 0.048  acc: 0.422 val loss: 1.763 val acc: 0.380\n",
      "\n",
      "At setup 6 taken Time is : 6.61s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 13\n",
      "ETA: 0.11 - epoch: 14 loss: 0.046  acc: 0.426 val loss: 1.701 val acc: 0.403\n",
      "\n",
      "At setup 6 taken Time is : 6.65s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 14\n",
      "ETA: 0.11 - epoch: 15 loss: 0.046  acc: 0.435 val loss: 1.592 val acc: 0.446\n",
      "\n",
      "At setup 6 taken Time is : 6.63s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 15\n",
      "ETA: 0.11 - epoch: 16 loss: 0.049  acc: 0.444 val loss: 1.543 val acc: 0.460\n",
      "\n",
      "At setup 6 taken Time is : 6.63s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 16\n",
      "ETA: 0.11 - epoch: 17 loss: 0.051  acc: 0.449 val loss: 1.444 val acc: 0.490\n",
      "\n",
      "At setup 6 taken Time is : 6.64s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 17\n",
      "ETA: 0.11 - epoch: 18 loss: 0.052  acc: 0.459 val loss: 1.402 val acc: 0.507\n",
      "\n",
      "At setup 6 taken Time is : 6.64s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 18\n",
      "ETA: 0.11 - epoch: 19 loss: 0.053  acc: 0.466 val loss: 1.340 val acc: 0.530\n",
      "\n",
      "At setup 6 taken Time is : 6.64s\n",
      "Best weight saved\n",
      "\n",
      "Start of epoch 19\n",
      "ETA: 0.11 - epoch: 20 loss: 0.050  acc: 0.473 val loss: 1.294 val acc: 0.544\n",
      "\n",
      "At setup 6 taken Time is : 6.73s\n",
      "Best weight saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for ip in range(7):\n",
    "    model.set_weights(fw) ## every one starts from same initial points\n",
    "    \n",
    "    if ip==0:\n",
    "        \n",
    "         tuned_parameter =  train_setup_baseline(train_dax, val_dx ,model,echp, ip)\n",
    "            \n",
    "    elif ip==1:\n",
    "        \n",
    "         tuned_parameter =  train_setup_gc (train_dax, val_dx ,model,echp, ip)   \n",
    "            \n",
    "    elif ip==2:\n",
    "        \n",
    "         tuned_parameter =  train_setup_agc (train_dax, val_dx ,model,echp, ip)\n",
    "            \n",
    "            \n",
    "    elif ip==3:\n",
    "        \n",
    "         tuned_parameter =  train_setup_sam (train_dax, val_dx ,model,echp, ip)            \n",
    "             \n",
    "    elif ip==4:\n",
    "        \n",
    "         tuned_parameter =  train_setup3 (train_dax, val_dx ,model,echp, ip)\n",
    "            \n",
    "    elif ip==5:\n",
    "        \n",
    "         tuned_parameter =  train_setup2 (train_dax, val_dx ,model,echp, ip)\n",
    "           \n",
    "        \n",
    "    else:\n",
    "         tuned_parameter = train_setup1(train_dax, val_dx ,model,echp, ip)\n",
    "            \n",
    "    hw.append(tuned_parameter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 32ms/step - loss: 1.5207 - accuracy: 0.4571\n",
      "0.45706528425216675\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1.5774 - accuracy: 0.4508\n",
      "0.45077869296073914\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 1.5234 - accuracy: 0.4596\n",
      "0.4596371054649353\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1.4875 - accuracy: 0.4898\n",
      "0.48978424072265625\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1.2612 - accuracy: 0.5661\n",
      "0.5660808682441711\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1.2625 - accuracy: 0.5687\n",
      "0.5686526894569397\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1.2843 - accuracy: 0.5581\n",
      "0.558079719543457\n"
     ]
    }
   ],
   "source": [
    "ac1 = list()\n",
    "ac2 = list()\n",
    "\n",
    "btsz = 512\n",
    "\n",
    "noi = 0\n",
    "\n",
    "nut = x_test\n",
    "\n",
    "\n",
    "for i in range(len(hw)):\n",
    "    model.set_weights(hw[i][0])\n",
    "    #modelo.save_weights('vg_bs'+str(i)+'.h5')\n",
    "    \n",
    "    los1, acu1 = model.evaluate(nut, y_test, batch_size=btsz)\n",
    "    print(acu1)\n",
    "    ac1.append(acu1)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# print(acu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvo1 = list()\n",
    "hvo1.append('Base') #0\n",
    "hvo1.append('GC')#1\n",
    "hvo1.append('AG Clipping')#2\n",
    "hvo1.append('SAM')#3\n",
    "hvo1.append('SAD_T1')#4\n",
    "\n",
    "hvo1.append('SAD_T2')#5\n",
    "hvo1.append('SAD_T3')#6\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1116x468 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 534,
       "width": 1178
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Visualizing the data\n",
    "fig = plt.figure(figsize=(15.5, 6.5))\n",
    "ax = fig.add_axes([0,0,1,1]) \n",
    "col_map = plt.get_cmap('tab20c')\n",
    "\n",
    "# Creating a bar chart from a dictionary\n",
    "plt.bar(hvo1, ac1, width=0.5, \n",
    "        color=col_map.colors, edgecolor='maroon', linewidth=.4)\n",
    "\n",
    "plt.title('Accuracy performance analysis', fontsize=15)\n",
    "plt.xlabel('Accuracies', fontsize=15)\n",
    "plt.ylabel('methods', fontsize=15)\n",
    "\n",
    "ax.set_ylim([np.min(ac1)-0.01, np.max(ac1)+0.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# nvo1 = list()\n",
    "# nvo1.append('VGG') #0\n",
    "# nvo1.append('InceptionResNet')#1\n",
    "# nvo1.append('Simple_cnn')#2\n",
    " \n",
    "########### change here\n",
    "# id2 = 1\n",
    "\n",
    "\n",
    "\n",
    "# sample_dir = 'nips_weights/'+ nvo1[id2]+'_batch_size_'+str(bzs) \n",
    "# if not os.path.isdir(sample_dir):\n",
    "#     os.makedirs(sample_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(hw)):\n",
    "#     model.set_weights(hw[i][0])\n",
    "#     model.save_weights(sample_dir+'/'+hvo1[i]+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:A_ten2.6]",
   "language": "python",
   "name": "conda-env-A_ten2.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
